{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "* Turn your classifier into a deep network\n",
    "* Use the optimizer to compute gradients\n",
    "* Understand regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models are Limited\n",
    "\n",
    "![image1.png](Images/Lesson2/image1.png)\n",
    "\n",
    "![image2.png](Images/Lesson2/image2.png)\n",
    "\n",
    "![image3.png](Images/Lesson2/image3.png)\n",
    "\n",
    "![image4.png](Images/Lesson2/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rectified Linear Units\n",
    "\n",
    "RELUs are the lazy engineer's solution to non-linearity. They are linear if x > 0, and 0 everywhere else. They also have very simple derivatives, just 0 if x < 0, and 1 everywhere else. "
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network with RELUs\n",
    "\n",
    "![image5.png](Images/Lesson2/image5.png)"
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Layer Neural Network\n",
    "\n",
    "![image6.png](Images/Lesson2/image6.png)\n",
    "\n",
    "### Multilayer Neural Networks\n",
    "In this lesson, you'll learn how to build multilayer neural networks with TensorFlow. Adding a hidden layer to a network allows it to model more complex functions. Also, using a non-linear activation function on the hidden layer lets it model non-linear functions.\n",
    "\n",
    "Next, you'll see how a ReLU hidden layer is implemented in TensorFlow.\n",
    "**Note:** Depicted above is a \"2-layer\" neural network:\n",
    "\n",
    "1. The first layer effectively consists of the set of weights and biases applied to X and passed through ReLUs. The output of this layer is fed to the next one, but is not observable outside the network, hence it is known as a hidden layer.\n",
    "2. The second layer consists of the weights and biases applied to these intermediate outputs, followed by the softmax function to generate probabilities."
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Propagation\n",
    "\n",
    "![image7.png](Images/Lesson2/image7.png)"
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Deep Learning Network\n",
    "\n",
    "![image8.png](Images/Lesson2/image8.png)\n",
    "\n",
    "It is typically better to go deep rather than wider in a neural network. Not only is it easier to train, but a lot of data typically has a naturally heirarchical structure that we can observe, and is captured naturally by deep nets."
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "Why didn't we discover deep models sooner? Lots of reasons! But mostly because we didn't have enough data to train them.\n",
    "\n",
    "The other reason is skinny jeans. Skinny jeans look great! But they are often hard to put on. So most people end up using bigger jeans. The same is true for data. While it is possible to use smaller neural networks for the dataset, it is typical for us to use networks that are too big first.\n",
    "\n",
    "Regularization: Applying artificial limitations to parameters to make them easier to train. Think stretch pants.\n",
    "\n",
    "L2 Regularization: Add another term to the loss that penalizes large weights. It's typically done by adding the L2-norm (See Euclidean Distance and p-norm) of your weights to the loss, multiplied by some constant B.\n",
    "\n",
    "![image9.png](Images/Lesson2/image9.png)\n",
    "\n",
    "![image10.png](Images/Lesson2/image10.png)"
   ]
  },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "\n",
    "Dropout is another technique for regularization. The basic concept is that you take connections between two layers of a neural network (sometimes called activations), and randomly select half of them to be turned off for a training example. Then, at the end, average all of the weights between the layers. This causes the network to develop a \"generalized\" idea of the training set, and naturally regularizes the training.\n",
    "\n",
    "![image11.png](Images/Lesson2/image11.png)\n",
    "\n",
    "![image12.png]Images/Lesson2/image12.png)\n",
    "\n",
    "![image13.png](Images/Lesson2/image13.png)\n",
    "\n",
    "![image14.png](Images/Lesson2/image14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
